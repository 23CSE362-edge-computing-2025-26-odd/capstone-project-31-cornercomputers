# Summary
Our project presents a novel EdgeAIGC framework designed to minimize latency and service costs associated with Artificial Intelligence Generated Content (AIGC) by moving computation from centralized cloud servers to distributed edge devices. The framework leverages the TD3 reinforcement learning algorithm to make intelligent decisions about model caching and multi-user resource allocation in real time, adapting to changing network conditions and model popularity. While a traffic management use case is being developed as a proof of concept, the primary focus so far has been on designing the architecture and validating the optimization strategy through simulation tools like iFogSim. 
# Feedback and Questions
1. Where is the generative part of the project mentioned in the paper?
	- Even though the paper focused on generative AI models, the framework has wider applications beyond Generative AI. Besides, our use case is not final due to certain issues with the availability of models
2. Criticism - Literature does not justify approach
3. Criticism - Make task split up better - documentation cannot be a sole task of a team member - solution is to split up roles to each person of team
4.   How is the reinforcement learning achieved?
	- Reinforcement learning in TD3 is achieved through an actor-critic framework. The actor network proposes actions based on the current state, while the critic networks evaluate these actions by estimating their Q-values. To improve stability, TD3 uses two critic networks and takes the minimum Q-value to avoid overestimation. The target networks (delayed copies of the actor and critic) are used to compute a more stable target Q-value for training. The agent then uses the Bellman equation to minimize the critic's error and update the actor via the policy gradient, encouraging actions that lead to higher estimated returns. This feedback loop of interaction, evaluation, and updating forms the core of reinforcement learning in TD3. 
	> TLDR: TD3 uses an actor-critic setup where the actor suggests actions and two critic networks evaluate them to avoid overestimation. Target networks help stabilize learning, and updates are done using the Bellman equation and policy gradients, forming a loop of learning from interaction.
5.  Why need edge AI architecture ?
	- edge AI is a way to lower the latency of response taken by AI and to minimize the bandwidth used. Making a low latency and almost locally run AI.
6.  Why take the minimum of the Q-values given by the Twin Critic network?
	- The DDPG (Deep Deterministic Policy Gradient) algorithm often suffers from overestimation bias, where the critic network predicts overly optimistic Q-values. This overestimation leads to poor policy updates and unstable training. TD3 (Twin Delayed DDPG) addresses this by using two independent critic networks and taking the minimum of their predicted Q-values when computing the target. This "clipped double Q-learning" approach provides a more conservative and reliable value estimate, reducing overestimation and improving training stability.